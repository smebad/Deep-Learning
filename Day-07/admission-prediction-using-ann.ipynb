{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Serial No.",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GRE Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOEFL Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "University Rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SOP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LOR ",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CGPA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Chance of Admit ",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d91f4dcf-b0a1-4a6b-94d7-a48802496592",
       "rows": [
        [
         "0",
         "1",
         "337",
         "118",
         "4",
         "4.5",
         "4.5",
         "9.65",
         "1",
         "0.92"
        ],
        [
         "1",
         "2",
         "324",
         "107",
         "4",
         "4.0",
         "4.5",
         "8.87",
         "1",
         "0.76"
        ],
        [
         "2",
         "3",
         "316",
         "104",
         "3",
         "3.0",
         "3.5",
         "8.0",
         "1",
         "0.72"
        ],
        [
         "3",
         "4",
         "322",
         "110",
         "3",
         "3.5",
         "2.5",
         "8.67",
         "1",
         "0.8"
        ],
        [
         "4",
         "5",
         "314",
         "103",
         "2",
         "2.0",
         "3.0",
         "8.21",
         "0",
         "0.65"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Admission_Predict_Ver1.1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 9)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         500 non-null    int64  \n",
      " 1   GRE Score          500 non-null    int64  \n",
      " 2   TOEFL Score        500 non-null    int64  \n",
      " 3   University Rating  500 non-null    int64  \n",
      " 4   SOP                500 non-null    float64\n",
      " 5   LOR                500 non-null    float64\n",
      " 6   CGPA               500 non-null    float64\n",
      " 7   Research           500 non-null    int64  \n",
      " 8   Chance of Admit    500 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 35.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial No.           0\n",
       "GRE Score            0\n",
       "TOEFL Score          0\n",
       "University Rating    0\n",
       "SOP                  0\n",
       "LOR                  0\n",
       "CGPA                 0\n",
       "Research             0\n",
       "Chance of Admit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GRE Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOEFL Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "University Rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SOP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LOR ",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CGPA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Chance of Admit ",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "622fce4d-8499-4f61-a81b-3409963f6488",
       "rows": [
        [
         "0",
         "337",
         "118",
         "4",
         "4.5",
         "4.5",
         "9.65",
         "1",
         "0.92"
        ],
        [
         "1",
         "324",
         "107",
         "4",
         "4.0",
         "4.5",
         "8.87",
         "1",
         "0.76"
        ],
        [
         "2",
         "316",
         "104",
         "3",
         "3.0",
         "3.5",
         "8.0",
         "1",
         "0.72"
        ],
        [
         "3",
         "322",
         "110",
         "3",
         "3.5",
         "2.5",
         "8.67",
         "1",
         "0.8"
        ],
        [
         "4",
         "314",
         "103",
         "2",
         "2.0",
         "3.0",
         "8.21",
         "0",
         "0.65"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0        337          118                  4  4.5   4.5  9.65         1   \n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping of the unnecessary column\n",
    "df.drop(['Serial No.'], inplace=True, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the independent and dependent variables\n",
    "X = df.iloc[:, 0:-1]  # independent variables\n",
    "y = df.iloc[:, -1]  # dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GRE Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOEFL Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "University Rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SOP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LOR ",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CGPA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e4e1ef30-6821-49dd-8341-19aa8fa65271",
       "rows": [
        [
         "0",
         "337",
         "118",
         "4",
         "4.5",
         "4.5",
         "9.65",
         "1"
        ],
        [
         "1",
         "324",
         "107",
         "4",
         "4.0",
         "4.5",
         "8.87",
         "1"
        ],
        [
         "2",
         "316",
         "104",
         "3",
         "3.0",
         "3.5",
         "8.0",
         "1"
        ],
        [
         "3",
         "322",
         "110",
         "3",
         "3.5",
         "2.5",
         "8.67",
         "1"
        ],
        [
         "4",
         "314",
         "103",
         "2",
         "2.0",
         "3.0",
         "8.21",
         "0"
        ],
        [
         "5",
         "330",
         "115",
         "5",
         "4.5",
         "3.0",
         "9.34",
         "1"
        ],
        [
         "6",
         "321",
         "109",
         "3",
         "3.0",
         "4.0",
         "8.2",
         "1"
        ],
        [
         "7",
         "308",
         "101",
         "2",
         "3.0",
         "4.0",
         "7.9",
         "0"
        ],
        [
         "8",
         "302",
         "102",
         "1",
         "2.0",
         "1.5",
         "8.0",
         "0"
        ],
        [
         "9",
         "323",
         "108",
         "3",
         "3.5",
         "3.0",
         "8.6",
         "0"
        ],
        [
         "10",
         "325",
         "106",
         "3",
         "3.5",
         "4.0",
         "8.4",
         "1"
        ],
        [
         "11",
         "327",
         "111",
         "4",
         "4.0",
         "4.5",
         "9.0",
         "1"
        ],
        [
         "12",
         "328",
         "112",
         "4",
         "4.0",
         "4.5",
         "9.1",
         "1"
        ],
        [
         "13",
         "307",
         "109",
         "3",
         "4.0",
         "3.0",
         "8.0",
         "1"
        ],
        [
         "14",
         "311",
         "104",
         "3",
         "3.5",
         "2.0",
         "8.2",
         "1"
        ],
        [
         "15",
         "314",
         "105",
         "3",
         "3.5",
         "2.5",
         "8.3",
         "0"
        ],
        [
         "16",
         "317",
         "107",
         "3",
         "4.0",
         "3.0",
         "8.7",
         "0"
        ],
        [
         "17",
         "319",
         "106",
         "3",
         "4.0",
         "3.0",
         "8.0",
         "1"
        ],
        [
         "18",
         "318",
         "110",
         "3",
         "4.0",
         "3.0",
         "8.8",
         "0"
        ],
        [
         "19",
         "303",
         "102",
         "3",
         "3.5",
         "3.0",
         "8.5",
         "0"
        ],
        [
         "20",
         "312",
         "107",
         "3",
         "3.0",
         "2.0",
         "7.9",
         "1"
        ],
        [
         "21",
         "325",
         "114",
         "4",
         "3.0",
         "2.0",
         "8.4",
         "0"
        ],
        [
         "22",
         "328",
         "116",
         "5",
         "5.0",
         "5.0",
         "9.5",
         "1"
        ],
        [
         "23",
         "334",
         "119",
         "5",
         "5.0",
         "4.5",
         "9.7",
         "1"
        ],
        [
         "24",
         "336",
         "119",
         "5",
         "4.0",
         "3.5",
         "9.8",
         "1"
        ],
        [
         "25",
         "340",
         "120",
         "5",
         "4.5",
         "4.5",
         "9.6",
         "1"
        ],
        [
         "26",
         "322",
         "109",
         "5",
         "4.5",
         "3.5",
         "8.8",
         "0"
        ],
        [
         "27",
         "298",
         "98",
         "2",
         "1.5",
         "2.5",
         "7.5",
         "1"
        ],
        [
         "28",
         "295",
         "93",
         "1",
         "2.0",
         "2.0",
         "7.2",
         "0"
        ],
        [
         "29",
         "310",
         "99",
         "2",
         "1.5",
         "2.0",
         "7.3",
         "0"
        ],
        [
         "30",
         "300",
         "97",
         "2",
         "3.0",
         "3.0",
         "8.1",
         "1"
        ],
        [
         "31",
         "327",
         "103",
         "3",
         "4.0",
         "4.0",
         "8.3",
         "1"
        ],
        [
         "32",
         "338",
         "118",
         "4",
         "3.0",
         "4.5",
         "9.4",
         "1"
        ],
        [
         "33",
         "340",
         "114",
         "5",
         "4.0",
         "4.0",
         "9.6",
         "1"
        ],
        [
         "34",
         "331",
         "112",
         "5",
         "4.0",
         "5.0",
         "9.8",
         "1"
        ],
        [
         "35",
         "320",
         "110",
         "5",
         "5.0",
         "5.0",
         "9.2",
         "1"
        ],
        [
         "36",
         "299",
         "106",
         "2",
         "4.0",
         "4.0",
         "8.4",
         "0"
        ],
        [
         "37",
         "300",
         "105",
         "1",
         "1.0",
         "2.0",
         "7.8",
         "0"
        ],
        [
         "38",
         "304",
         "105",
         "1",
         "3.0",
         "1.5",
         "7.5",
         "0"
        ],
        [
         "39",
         "307",
         "108",
         "2",
         "4.0",
         "3.5",
         "7.7",
         "0"
        ],
        [
         "40",
         "308",
         "110",
         "3",
         "3.5",
         "3.0",
         "8.0",
         "1"
        ],
        [
         "41",
         "316",
         "105",
         "2",
         "2.5",
         "2.5",
         "8.2",
         "1"
        ],
        [
         "42",
         "313",
         "107",
         "2",
         "2.5",
         "2.0",
         "8.5",
         "1"
        ],
        [
         "43",
         "332",
         "117",
         "4",
         "4.5",
         "4.0",
         "9.1",
         "0"
        ],
        [
         "44",
         "326",
         "113",
         "5",
         "4.5",
         "4.0",
         "9.4",
         "1"
        ],
        [
         "45",
         "322",
         "110",
         "5",
         "5.0",
         "4.0",
         "9.1",
         "1"
        ],
        [
         "46",
         "329",
         "114",
         "5",
         "4.0",
         "5.0",
         "9.3",
         "1"
        ],
        [
         "47",
         "339",
         "119",
         "5",
         "4.5",
         "4.0",
         "9.7",
         "0"
        ],
        [
         "48",
         "321",
         "110",
         "3",
         "3.5",
         "5.0",
         "8.85",
         "1"
        ],
        [
         "49",
         "327",
         "111",
         "4",
         "3.0",
         "4.0",
         "8.4",
         "1"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 500
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "0          337          118                  4  4.5   4.5  9.65         1\n",
       "1          324          107                  4  4.0   4.5  8.87         1\n",
       "2          316          104                  3  3.0   3.5  8.00         1\n",
       "3          322          110                  3  3.5   2.5  8.67         1\n",
       "4          314          103                  2  2.0   3.0  8.21         0\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "495        332          108                  5  4.5   4.0  9.02         1\n",
       "496        337          117                  5  5.0   5.0  9.87         1\n",
       "497        330          120                  5  4.5   5.0  9.56         1\n",
       "498        312          103                  4  4.0   5.0  8.43         0\n",
       "499        327          113                  4  4.5   4.5  9.04         0\n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.92\n",
       "1      0.76\n",
       "2      0.72\n",
       "3      0.80\n",
       "4      0.65\n",
       "       ... \n",
       "495    0.87\n",
       "496    0.96\n",
       "497    0.93\n",
       "498    0.73\n",
       "499    0.84\n",
       "Name: Chance of Admit , Length: 500, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GRE Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOEFL Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "University Rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SOP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LOR ",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CGPA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1bc85ef2-c3fd-4cbd-b84e-5842e2216e1a",
       "rows": [
        [
         "238",
         "310",
         "104",
         "3",
         "2.0",
         "3.5",
         "8.37",
         "0"
        ],
        [
         "438",
         "318",
         "110",
         "1",
         "2.5",
         "3.5",
         "8.54",
         "1"
        ],
        [
         "475",
         "300",
         "101",
         "3",
         "3.5",
         "2.5",
         "7.88",
         "0"
        ],
        [
         "58",
         "300",
         "99",
         "1",
         "3.0",
         "2.0",
         "6.8",
         "1"
        ],
        [
         "380",
         "322",
         "104",
         "3",
         "3.5",
         "4.0",
         "8.84",
         "1"
        ],
        [
         "78",
         "296",
         "95",
         "2",
         "3.0",
         "2.0",
         "7.54",
         "1"
        ],
        [
         "379",
         "311",
         "99",
         "1",
         "2.5",
         "3.0",
         "8.43",
         "1"
        ],
        [
         "247",
         "311",
         "104",
         "2",
         "2.5",
         "3.5",
         "8.48",
         "0"
        ],
        [
         "207",
         "310",
         "102",
         "3",
         "3.5",
         "4.0",
         "8.02",
         "1"
        ],
        [
         "101",
         "312",
         "105",
         "2",
         "2.5",
         "3.0",
         "8.12",
         "0"
        ],
        [
         "344",
         "295",
         "96",
         "2",
         "1.5",
         "2.0",
         "7.34",
         "0"
        ],
        [
         "392",
         "326",
         "112",
         "4",
         "4.0",
         "3.5",
         "9.12",
         "1"
        ],
        [
         "423",
         "334",
         "119",
         "5",
         "4.5",
         "5.0",
         "9.54",
         "1"
        ],
        [
         "488",
         "322",
         "112",
         "3",
         "3.0",
         "4.0",
         "8.62",
         "1"
        ],
        [
         "256",
         "309",
         "99",
         "3",
         "4.0",
         "4.0",
         "8.56",
         "0"
        ],
        [
         "180",
         "300",
         "104",
         "3",
         "3.5",
         "3.0",
         "8.16",
         "0"
        ],
        [
         "436",
         "310",
         "110",
         "1",
         "1.5",
         "4.0",
         "7.23",
         "1"
        ],
        [
         "18",
         "318",
         "110",
         "3",
         "4.0",
         "3.0",
         "8.8",
         "0"
        ],
        [
         "40",
         "308",
         "110",
         "3",
         "3.5",
         "3.0",
         "8.0",
         "1"
        ],
        [
         "421",
         "321",
         "112",
         "3",
         "3.0",
         "4.5",
         "8.95",
         "1"
        ],
        [
         "469",
         "326",
         "114",
         "4",
         "4.0",
         "3.5",
         "9.16",
         "1"
        ],
        [
         "41",
         "316",
         "105",
         "2",
         "2.5",
         "2.5",
         "8.2",
         "1"
        ],
        [
         "159",
         "297",
         "100",
         "1",
         "1.5",
         "2.0",
         "7.9",
         "0"
        ],
        [
         "286",
         "336",
         "118",
         "5",
         "4.5",
         "4.0",
         "9.19",
         "1"
        ],
        [
         "132",
         "309",
         "105",
         "5",
         "3.5",
         "3.5",
         "8.56",
         "0"
        ],
        [
         "290",
         "307",
         "105",
         "2",
         "2.5",
         "3.0",
         "7.65",
         "0"
        ],
        [
         "496",
         "337",
         "117",
         "5",
         "5.0",
         "5.0",
         "9.87",
         "1"
        ],
        [
         "341",
         "326",
         "110",
         "3",
         "3.5",
         "3.5",
         "8.76",
         "1"
        ],
        [
         "80",
         "312",
         "105",
         "3",
         "2.0",
         "3.0",
         "8.02",
         "1"
        ],
        [
         "46",
         "329",
         "114",
         "5",
         "4.0",
         "5.0",
         "9.3",
         "1"
        ],
        [
         "456",
         "299",
         "100",
         "2",
         "2.0",
         "2.0",
         "7.88",
         "0"
        ],
        [
         "93",
         "301",
         "97",
         "2",
         "3.0",
         "3.0",
         "7.88",
         "1"
        ],
        [
         "446",
         "327",
         "118",
         "4",
         "5.0",
         "5.0",
         "9.67",
         "1"
        ],
        [
         "81",
         "340",
         "120",
         "4",
         "5.0",
         "5.0",
         "9.5",
         "1"
        ],
        [
         "372",
         "336",
         "119",
         "4",
         "4.5",
         "4.0",
         "9.62",
         "1"
        ],
        [
         "325",
         "326",
         "116",
         "3",
         "3.5",
         "4.0",
         "9.14",
         "1"
        ],
        [
         "451",
         "324",
         "113",
         "4",
         "4.5",
         "4.5",
         "9.25",
         "1"
        ],
        [
         "454",
         "310",
         "105",
         "2",
         "3.0",
         "3.5",
         "8.01",
         "0"
        ],
        [
         "375",
         "304",
         "101",
         "2",
         "2.0",
         "2.5",
         "7.66",
         "0"
        ],
        [
         "225",
         "296",
         "99",
         "2",
         "2.5",
         "2.5",
         "8.03",
         "0"
        ],
        [
         "139",
         "318",
         "109",
         "1",
         "3.5",
         "3.5",
         "9.12",
         "0"
        ],
        [
         "106",
         "329",
         "111",
         "4",
         "4.5",
         "4.5",
         "9.18",
         "1"
        ],
        [
         "232",
         "312",
         "107",
         "2",
         "2.5",
         "3.5",
         "8.27",
         "0"
        ],
        [
         "291",
         "300",
         "102",
         "2",
         "1.5",
         "2.0",
         "7.87",
         "0"
        ],
        [
         "204",
         "298",
         "105",
         "3",
         "3.5",
         "4.0",
         "8.54",
         "0"
        ],
        [
         "331",
         "311",
         "105",
         "2",
         "3.0",
         "2.0",
         "8.12",
         "1"
        ],
        [
         "197",
         "310",
         "106",
         "2",
         "3.5",
         "2.5",
         "8.33",
         "0"
        ],
        [
         "95",
         "304",
         "100",
         "4",
         "1.5",
         "2.5",
         "7.84",
         "0"
        ],
        [
         "122",
         "310",
         "106",
         "4",
         "1.5",
         "2.5",
         "8.36",
         "0"
        ],
        [
         "23",
         "334",
         "119",
         "5",
         "5.0",
         "4.5",
         "9.7",
         "1"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 400
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>310</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>318</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>300</td>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>300</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>322</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>307</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>321</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>326</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>300</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "238        310          104                  3  2.0   3.5  8.37         0\n",
       "438        318          110                  1  2.5   3.5  8.54         1\n",
       "475        300          101                  3  3.5   2.5  7.88         0\n",
       "58         300           99                  1  3.0   2.0  6.80         1\n",
       "380        322          104                  3  3.5   4.0  8.84         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "255        307          110                  4  4.0   4.5  8.37         0\n",
       "72         321          111                  5  5.0   5.0  9.45         1\n",
       "396        325          107                  3  3.0   3.5  9.11         1\n",
       "235        326          111                  5  4.5   4.0  9.23         1\n",
       "37         300          105                  1  1.0   2.0  7.80         0\n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4       , 0.42857143, 0.5       , ..., 0.57142857, 0.50320513,\n",
       "        0.        ],\n",
       "       [0.56      , 0.64285714, 0.        , ..., 0.57142857, 0.55769231,\n",
       "        1.        ],\n",
       "       [0.2       , 0.32142857, 0.5       , ..., 0.28571429, 0.34615385,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.7       , 0.53571429, 0.5       , ..., 0.57142857, 0.74038462,\n",
       "        1.        ],\n",
       "       [0.72      , 0.67857143, 1.        , ..., 0.71428571, 0.77884615,\n",
       "        1.        ],\n",
       "       [0.2       , 0.46428571, 0.        , ..., 0.14285714, 0.32051282,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler # Normalization\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train) # fit and transform the training data\n",
    "X_test_scaled = scaler.transform(X_test) # transform the testing data\n",
    "\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building up the neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DC\\miniconda3\\envs\\TF_env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Sequential()\n",
    "\n",
    "# adding the input layer\n",
    "model.add(Dense(7, activation='relu', input_dim=7)) # 7 neurons in the input layer\n",
    "\n",
    "# adding the hidden layers\n",
    "model.add(Dense(7, activation='relu')) # 7 neurons in the first hidden layer\n",
    "\n",
    "# adding the output layer\n",
    "model.add(Dense(1, activation='linear')) # 1 neuron in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m8\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.5953 - val_loss: 0.5626\n",
      "Epoch 2/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4922 - val_loss: 0.4747\n",
      "Epoch 3/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4052 - val_loss: 0.4055\n",
      "Epoch 4/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3519 - val_loss: 0.3268\n",
      "Epoch 5/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2808 - val_loss: 0.2294\n",
      "Epoch 6/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1896 - val_loss: 0.1240\n",
      "Epoch 7/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0938 - val_loss: 0.0589\n",
      "Epoch 8/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0444 - val_loss: 0.0351\n",
      "Epoch 9/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0308 - val_loss: 0.0304\n",
      "Epoch 10/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0271 - val_loss: 0.0272\n",
      "Epoch 11/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0217 - val_loss: 0.0229\n",
      "Epoch 12/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0213 - val_loss: 0.0184\n",
      "Epoch 13/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0134 - val_loss: 0.0143\n",
      "Epoch 14/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0118 - val_loss: 0.0115\n",
      "Epoch 15/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 16/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0097 - val_loss: 0.0087\n",
      "Epoch 17/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0073 - val_loss: 0.0081\n",
      "Epoch 18/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0070 - val_loss: 0.0076\n",
      "Epoch 19/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0061 - val_loss: 0.0073\n",
      "Epoch 20/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 21/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 22/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0050 - val_loss: 0.0068\n",
      "Epoch 23/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0062 - val_loss: 0.0068\n",
      "Epoch 24/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 25/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0058 - val_loss: 0.0065\n",
      "Epoch 26/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0060 - val_loss: 0.0064\n",
      "Epoch 27/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0058 - val_loss: 0.0063\n",
      "Epoch 28/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0057 - val_loss: 0.0062\n",
      "Epoch 29/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0058 - val_loss: 0.0062\n",
      "Epoch 30/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0049 - val_loss: 0.0060\n",
      "Epoch 31/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0045 - val_loss: 0.0060\n",
      "Epoch 32/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0050 - val_loss: 0.0060\n",
      "Epoch 33/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0056 - val_loss: 0.0059\n",
      "Epoch 34/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0053 - val_loss: 0.0058\n",
      "Epoch 35/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0052 - val_loss: 0.0059\n",
      "Epoch 36/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 37/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0047 - val_loss: 0.0058\n",
      "Epoch 38/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0049 - val_loss: 0.0057\n",
      "Epoch 39/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0049 - val_loss: 0.0056\n",
      "Epoch 40/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0049 - val_loss: 0.0056\n",
      "Epoch 41/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 42/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0056\n",
      "Epoch 43/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0045 - val_loss: 0.0055\n",
      "Epoch 44/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 45/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 46/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 47/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0046 - val_loss: 0.0053\n",
      "Epoch 48/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 49/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0045 - val_loss: 0.0053\n",
      "Epoch 50/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0047 - val_loss: 0.0052\n",
      "Epoch 51/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0040 - val_loss: 0.0052\n",
      "Epoch 52/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 53/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 54/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0049 - val_loss: 0.0052\n",
      "Epoch 55/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.0051\n",
      "Epoch 56/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.0052\n",
      "Epoch 57/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0050\n",
      "Epoch 58/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0051\n",
      "Epoch 59/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0050\n",
      "Epoch 60/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0050\n",
      "Epoch 61/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0049\n",
      "Epoch 62/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0044 - val_loss: 0.0050\n",
      "Epoch 63/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0041 - val_loss: 0.0049\n",
      "Epoch 64/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0039 - val_loss: 0.0050\n",
      "Epoch 65/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0043 - val_loss: 0.0049\n",
      "Epoch 66/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 67/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 68/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.0048\n",
      "Epoch 69/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.0048\n",
      "Epoch 70/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 71/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 72/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.0047\n",
      "Epoch 73/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0048\n",
      "Epoch 74/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0046\n",
      "Epoch 75/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0047\n",
      "Epoch 76/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 77/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0046\n",
      "Epoch 78/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 79/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0046\n",
      "Epoch 80/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0046\n",
      "Epoch 81/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0044 - val_loss: 0.0046\n",
      "Epoch 82/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0045\n",
      "Epoch 83/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0045\n",
      "Epoch 84/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 85/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0041 - val_loss: 0.0045\n",
      "Epoch 86/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 87/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 88/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0039 - val_loss: 0.0045\n",
      "Epoch 89/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 90/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 91/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0038 - val_loss: 0.0044\n",
      "Epoch 92/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 93/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 94/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0044\n",
      "Epoch 95/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 96/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0038 - val_loss: 0.0044\n",
      "Epoch 97/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0037 - val_loss: 0.0044\n",
      "Epoch 98/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 99/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 100/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 101/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 102/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 103/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 104/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 105/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0044\n",
      "Epoch 106/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 107/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 108/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 109/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 110/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 111/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 112/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 113/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 114/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0030 - val_loss: 0.0043\n",
      "Epoch 115/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 116/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 117/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 118/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 119/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 120/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 121/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 122/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 123/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 124/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 125/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 126/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 127/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 128/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 129/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 130/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 131/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 132/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 133/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 134/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 135/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 136/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0038 - val_loss: 0.0041\n",
      "Epoch 137/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 138/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 139/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 140/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 141/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 142/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 143/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 144/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 145/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 146/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 147/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0040\n",
      "Epoch 148/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 149/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 150/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 151/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 152/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 153/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 154/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 155/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 156/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 157/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 158/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0041\n",
      "Epoch 159/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 160/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 161/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 162/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 163/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 164/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 165/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 166/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 167/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 168/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 169/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 170/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 171/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 172/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 173/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 174/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 175/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 176/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0028 - val_loss: 0.0041\n",
      "Epoch 177/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 178/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 179/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0036 - val_loss: 0.0041\n",
      "Epoch 180/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 181/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 182/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 183/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 184/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0041\n",
      "Epoch 185/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 186/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 187/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 188/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 189/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 190/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 191/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 192/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0036 - val_loss: 0.0040\n",
      "Epoch 193/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0038 - val_loss: 0.0041\n",
      "Epoch 194/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0027 - val_loss: 0.0040\n",
      "Epoch 195/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0041\n",
      "Epoch 196/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0036 - val_loss: 0.0040\n",
      "Epoch 197/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 198/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 199/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 200/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0034 - val_loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=200, validation_split=0.2) # 20% of the training data will be used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DB95109E40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/stepWARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DB95109E40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.64283544],\n",
       "       [0.7157303 ],\n",
       "       [0.8861298 ],\n",
       "       [0.7218029 ],\n",
       "       [0.81810015],\n",
       "       [0.6609894 ],\n",
       "       [0.7404403 ],\n",
       "       [0.6922397 ],\n",
       "       [0.7867784 ],\n",
       "       [0.60601103],\n",
       "       [0.67775273],\n",
       "       [0.4943835 ],\n",
       "       [0.78854   ],\n",
       "       [0.77404547],\n",
       "       [0.769254  ],\n",
       "       [0.8715715 ],\n",
       "       [0.6193784 ],\n",
       "       [0.75153726],\n",
       "       [0.9091788 ],\n",
       "       [0.6247327 ],\n",
       "       [0.60468626],\n",
       "       [0.7600144 ],\n",
       "       [0.8345026 ],\n",
       "       [0.521046  ],\n",
       "       [0.7506131 ],\n",
       "       [0.57949454],\n",
       "       [0.9530769 ],\n",
       "       [0.6323839 ],\n",
       "       [0.86668885],\n",
       "       [0.70900834],\n",
       "       [0.6105847 ],\n",
       "       [0.80784756],\n",
       "       [0.6117881 ],\n",
       "       [0.9043615 ],\n",
       "       [0.5022487 ],\n",
       "       [0.81118125],\n",
       "       [0.6776737 ],\n",
       "       [0.634557  ],\n",
       "       [0.6632189 ],\n",
       "       [0.90484554],\n",
       "       [0.51078737],\n",
       "       [0.62699765],\n",
       "       [0.7842441 ],\n",
       "       [0.9764122 ],\n",
       "       [0.76595265],\n",
       "       [0.50641817],\n",
       "       [0.6701504 ],\n",
       "       [0.6411739 ],\n",
       "       [0.656702  ],\n",
       "       [0.66348237],\n",
       "       [0.82845324],\n",
       "       [0.9118913 ],\n",
       "       [0.8925628 ],\n",
       "       [0.624234  ],\n",
       "       [0.7616264 ],\n",
       "       [0.62484074],\n",
       "       [0.7155845 ],\n",
       "       [0.61242825],\n",
       "       [0.6600564 ],\n",
       "       [0.70025724],\n",
       "       [0.44787562],\n",
       "       [0.74983037],\n",
       "       [0.72881806],\n",
       "       [0.8487813 ],\n",
       "       [0.97784305],\n",
       "       [0.62227446],\n",
       "       [0.7296131 ],\n",
       "       [0.78849787],\n",
       "       [0.91279966],\n",
       "       [0.6854474 ],\n",
       "       [0.59089833],\n",
       "       [0.6250743 ],\n",
       "       [0.8208119 ],\n",
       "       [0.48898053],\n",
       "       [0.9111989 ],\n",
       "       [0.61241305],\n",
       "       [0.8356737 ],\n",
       "       [0.95478255],\n",
       "       [0.710376  ],\n",
       "       [0.7909183 ],\n",
       "       [0.8427153 ],\n",
       "       [0.48801988],\n",
       "       [0.9227875 ],\n",
       "       [0.78241044],\n",
       "       [0.78014416],\n",
       "       [0.71050686],\n",
       "       [0.8947142 ],\n",
       "       [0.8986475 ],\n",
       "       [0.5597358 ],\n",
       "       [0.55828947],\n",
       "       [0.5948884 ],\n",
       "       [0.7679437 ],\n",
       "       [0.5817686 ],\n",
       "       [0.7067622 ],\n",
       "       [0.8165959 ],\n",
       "       [0.83238333],\n",
       "       [0.85321635],\n",
       "       [0.52911747],\n",
       "       [0.7392015 ],\n",
       "       [0.6110929 ]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting the test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8253364073538794"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the model\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABND0lEQVR4nO3dd3wUdf4/8Ndsz6ZDSAEjkaKAQJAWY0GUaEBOQVEjcgI5hFPK4eW8Qw6pngZFOb4KBzbADuJPlLOAEMFGlCZViMIBicImIKYnu5udz++PLWRJCCmzO8nm9Xw89sHuzGdm3pMB9pXPZ4okhBAgIiIiChAatQsgIiIiUhLDDREREQUUhhsiIiIKKAw3REREFFAYboiIiCigMNwQERFRQGG4ISIiooDCcENEREQBheGGiIiIAgrDDREREQUUhhsiqtPq1ashSRJ27dqldin1snfvXvzxj39EfHw8jEYj2rRpg5SUFKxatQoOh0Pt8ojID3RqF0BEpJRXX30VDz/8MGJiYvDggw+ia9euKCkpQVZWFiZMmIDTp0/jn//8p9plEpGPMdwQUUD47rvv8PDDDyM5ORmffvopQkNDPfMeffRR7Nq1CwcPHlRkW2VlZQgODlZkXUSkPA5LEZEifvjhBwwbNgxhYWEICQnBkCFD8N1333m1sdvtmD9/Prp27QqTyYS2bdvihhtuwObNmz1tLBYL0tPTcdlll8FoNCIuLg4jRozAiRMn6tz+/PnzIUkS3n77ba9g49a/f3+MHz8eALBt2zZIkoRt27Z5tTlx4gQkScLq1as908aPH4+QkBAcO3YMt99+O0JDQzFmzBhMnToVISEhKC8vr7Gt0aNHIzY21msY7LPPPsONN96I4OBghIaGYvjw4Th06FCd+0REjcNwQ0RNdujQIdx4443Yt28f/vGPf2D27Nk4fvw4Bg8ejO+//97Tbt68eZg/fz5uvvlmLF26FLNmzcLll1+OPXv2eNqMGjUK69evR3p6Ov7zn//gL3/5C0pKSpCbm3vR7ZeXlyMrKwuDBg3C5Zdfrvj+VVVVITU1FdHR0XjuuecwatQopKWloaysDJ988kmNWv773//innvugVarBQC8+eabGD58OEJCQvDMM89g9uzZ+PHHH3HDDTdcMrQRUSMIIqI6rFq1SgAQO3fuvGibkSNHCoPBII4dO+aZdurUKREaGioGDRrkmZaYmCiGDx9+0fX8/vvvAoBYtGhRg2rct2+fACCmT59er/Zbt24VAMTWrVu9ph8/flwAEKtWrfJMGzdunAAgHn/8ca+2siyLDh06iFGjRnlNf++99wQA8dVXXwkhhCgpKRERERFi4sSJXu0sFosIDw+vMZ2Imo49N0TUJA6HA59//jlGjhyJTp06eabHxcXhgQcewDfffIPi4mIAQEREBA4dOoSff/651nUFBQXBYDBg27Zt+P333+tdg3v9tQ1HKeWRRx7x+ixJEu699158+umnKC0t9Uxfu3YtOnTogBtuuAEAsHnzZhQWFmL06NE4e/as56XVapGUlIStW7f6rGai1orhhoia5MyZMygvL8dVV11VY1737t0hyzLy8vIAAAsWLEBhYSGuvPJK9OrVC3//+9+xf/9+T3uj0YhnnnkGn332GWJiYjBo0CA8++yzsFgsddYQFhYGACgpKVFwz87T6XS47LLLakxPS0tDRUUFNmzYAAAoLS3Fp59+invvvReSJAGAJ8jdcsstaNeundfr888/R0FBgU9qJmrNGG6IyG8GDRqEY8eOYeXKlejZsydeffVV9O3bF6+++qqnzaOPPoqffvoJmZmZMJlMmD17Nrp3744ffvjhouvt0qULdDodDhw4UK863MHjQhe7D47RaIRGU/O/y2uvvRYJCQl47733AAD//e9/UVFRgbS0NE8bWZYBOM+72bx5c43XRx99VK+aiaj+GG6IqEnatWsHs9mMnJycGvOOHDkCjUaD+Ph4z7Q2bdogPT0d7777LvLy8tC7d2/MmzfPa7nOnTvjb3/7Gz7//HMcPHgQNpsNzz///EVrMJvNuOWWW/DVV195eonqEhkZCQAoLCz0mn7y5MlLLnuh++67Dxs3bkRxcTHWrl2LhIQEXHvttV77AgDR0dFISUmp8Ro8eHCDt0lEdWO4IaIm0Wq1uO222/DRRx95XfmTn5+Pd955BzfccINn2Oi3337zWjYkJARdunSB1WoF4LzSqLKy0qtN586dERoa6mlzMXPnzoUQAg8++KDXOTBuu3fvxuuvvw4A6NixI7RaLb766iuvNv/5z3/qt9PVpKWlwWq14vXXX8fGjRtx3333ec1PTU1FWFgYnn76adjt9hrLnzlzpsHbJKK68SZ+RFQvK1euxMaNG2tMnz59Ov71r39h8+bNuOGGGzB58mTodDq89NJLsFqtePbZZz1te/TogcGDB6Nfv35o06YNdu3ahffffx9Tp04FAPz0008YMmQI7rvvPvTo0QM6nQ7r169Hfn4+7r///jrru+6667Bs2TJMnjwZ3bp187pD8bZt27Bhwwb861//AgCEh4fj3nvvxYsvvghJktC5c2d8/PHHjTr/pW/fvujSpQtmzZoFq9XqNSQFOM8HWr58OR588EH07dsX999/P9q1a4fc3Fx88sknuP7667F06dIGb5eI6qD25VpE1Ly5LwW/2CsvL08IIcSePXtEamqqCAkJEWazWdx8881i+/btXuv617/+JQYOHCgiIiJEUFCQ6Natm3jqqaeEzWYTQghx9uxZMWXKFNGtWzcRHBwswsPDRVJSknjvvffqXe/u3bvFAw88INq3by/0er2IjIwUQ4YMEa+//rpwOByedmfOnBGjRo0SZrNZREZGij//+c/i4MGDtV4KHhwcXOc2Z82aJQCILl26XLTN1q1bRWpqqggPDxcmk0l07txZjB8/Xuzatave+0ZE9SMJIYRqyYqIiIhIYTznhoiIiAIKww0REREFFIYbIiIiCigMN0RERBRQGG6IiIgooDDcEBERUUBpdTfxk2UZp06dQmho6EWfL0NERETNixACJSUlaN++fa3Pequu1YWbU6dOeT3nhoiIiFqOvLw8XHbZZXW2aXXhJjQ0FIDzh+N+3g0RERE1b8XFxYiPj/d8j9dF9XCzbNkyLFq0CBaLBYmJiXjxxRcxcODAi7YvLCzErFmz8MEHH+DcuXPo2LEjlixZgttvv71e23MPRYWFhTHcEBERtTD1OaVE1XCzdu1aZGRkYMWKFUhKSsKSJUuQmpqKnJwcREdH12hvs9lw6623Ijo6Gu+//z46dOiAkydPIiIiwv/FExERUbOk6rOlkpKSMGDAAM8TcWVZRnx8PKZNm4bHH3+8RvsVK1Zg0aJFOHLkCPR6faO2WVxcjPDwcBQVFbHnhoiIqIVoyPe3apeC22w27N69GykpKeeL0WiQkpKC7OzsWpfZsGEDkpOTMWXKFMTExKBnz554+umn4XA4/FU2ERERNXOqDUudPXsWDocDMTExXtNjYmJw5MiRWpf53//+hy+++AJjxozBp59+iqNHj2Ly5Mmw2+2YO3durctYrVZYrVbP5+Li4nrV53A4YLfb67k31Jzp9XpotVq1yyAiIj9R/YTihpBlGdHR0Xj55Zeh1WrRr18//Prrr1i0aNFFw01mZibmz59f720IIWCxWFBYWKhQ1dQcREREIDY2lvc2IiJqBVQLN1FRUdBqtcjPz/eanp+fj9jY2FqXiYuLq/FbePfu3WGxWGCz2WAwGGosM3PmTGRkZHg+uy8luxh3sImOjobZbOaXYQsnhEB5eTkKCgoAOP8OERFRYFMt3BgMBvTr1w9ZWVkYOXIkAGfPTFZWFqZOnVrrMtdffz3eeecdyLLsuTvhTz/9hLi4uFqDDQAYjUYYjcZ61eRwODzBpm3btg3fKWqWgoKCAAAFBQWIjo7mEBURUYBT9dlSGRkZeOWVV/D666/j8OHDeOSRR1BWVob09HQAwNixYzFz5kxP+0ceeQTnzp3D9OnT8dNPP+GTTz7B008/jSlTpihSj/scG7PZrMj6qPlwH1OeR0VEFPhUPecmLS0NZ86cwZw5c2CxWNCnTx9s3LjRc5Jxbm6u1/Mj4uPjsWnTJvz1r39F79690aFDB0yfPh0zZsxQtC4ORQUeHlMiotZD1fvcqKGu6+QrKytx/PhxXHHFFTCZTCpVSL7AY0tE1LK1iPvcUPOXkJCAJUuWqF0GERFRgzDcBABJkup8zZs3r1Hr3blzJyZNmqRssURERD7Wou5z05zJQqDKIQAIGHT+vRrn9OnTnvdr167FnDlzkJOT45kWEhLieS+EgMPhgE536UPfrl07ZQslIiLyA/bcKKTC5sARSzGOny33+7ZjY2M9r/DwcEiS5Pl85MgRhIaG4rPPPkO/fv1gNBrxzTff4NixYxgxYgRiYmIQEhKCAQMGYMuWLV7rvXBYSpIkvPrqq7jrrrtgNpvRtWtXbNiwwc97S0REVDeGm0sQQqDcVnXJV4W9CpV2R73a1vel5Lnejz/+OBYuXIjDhw+jd+/eKC0txe23346srCz88MMPGDp0KO644w7k5ubWuZ758+fjvvvuw/79+3H77bdjzJgxOHfunGJ1EhERNRWHpS6hwu5AjzmbVNn2jwtSYTYoc4gWLFiAW2+91fO5TZs2SExM9Hx+8sknsX79emzYsOGiN1EEgPHjx2P06NEAgKeffhovvPACduzYgaFDhypSJxERUVOx56aV6N+/v9fn0tJSPPbYY+jevTsiIiIQEhKCw4cPX7Lnpnfv3p73wcHBCAsL8zzagIiIqDlgz80lBOm1+HFB6iXb2R0yciwlAICr24cpctO4IL1yJyYHBwd7fX7sscewefNmPPfcc+jSpQuCgoJwzz33wGaz1bkevV7v9VmSJMiyrFidRERETcVwcwmSJNVraKhKlmFyhZEggw6aZn5H3G+//Rbjx4/HXXfdBcDZk3PixAl1iyIiIlIAh6UUUj3MyC3gps9du3bFBx98gL1792Lfvn144IEH2ANDREQBgeFGIdX7aVpAtsHixYsRGRmJ6667DnfccQdSU1PRt29ftcsiIiJqMj5bqpqmPn/o4K9FkIVAt9hQv9/Ij+rGZ0sREbVsfLaUSjSu7hu5VcVFIiKi5oXhRkHuK6Rawjk3REREgYpXSymlyooYnIVN0kCIkEu3JyIiIp9guFGKXIU2oghW6GBjzw0REZFqOCylFMn5o9RAtIirpYiIiAIVw41SPOFG5jk3REREKmK4UYor3GglwXBDRESkIoYbpUjnf5SCd/olIiJSDcONUqqFGwiGGyIiIrUw3ChFkiC7fpxCdqhcTMMNHjwYjz76qOdzQkIClixZUucykiThww8/bPK2lVoPERERwHCjKOF+wpSfe27uuOMODB06tNZ5X3/9NSRJwv79+xu0zp07d2LSpElKlOcxb9489OnTp8b006dPY9iwYYpui4iIWi+GGwUJ99CUn8PNhAkTsHnzZvzyyy815q1atQr9+/dH7969G7TOdu3awWw2K1VinWJjY2E0Gv2yLSIiCnwMNwpSK9z84Q9/QLt27bB69Wqv6aWlpVi3bh1GjhyJ0aNHo0OHDjCbzejVqxfefffdOtd54bDUzz//jEGDBsFkMqFHjx7YvHlzjWVmzJiBK6+8EmazGZ06dcLs2bNht9sBAKtXr8b8+fOxb98+SJIESZI89V44LHXgwAHccsstCAoKQtu2bTFp0iSUlpZ65o8fPx4jR47Ec889h7i4OLRt2xZTpkzxbIuIiFo33qH4UoQA7OX1a2u3AnIFgFLAFtT0bevNgOt5VXXR6XQYO3YsVq9ejVmzZnmecbVu3To4HA788Y9/xLp16zBjxgyEhYXhk08+wYMPPojOnTtj4MCBl1y/LMu4++67ERMTg++//x5FRUVe5+e4hYaGYvXq1Wjfvj0OHDiAiRMnIjQ0FP/4xz+QlpaGgwcPYuPGjdiyZQsAIDw8vMY6ysrKkJqaiuTkZOzcuRMFBQV46KGHMHXqVK/wtnXrVsTFxWHr1q04evQo0tLS0KdPH0ycOPGS+0NERIGN4eZS7OXA0+3r1VTv+jNKqW3/8xRgCK5X0z/96U9YtGgRvvzySwwePBiAc0hq1KhR6NixIx577DFP22nTpmHTpk1477336hVutmzZgiNHjmDTpk1o3975s3j66adrnCfzxBNPeN4nJCTgsccew5o1a/CPf/wDQUFBCAkJgU6nQ2xs7EW39c4776CyshJvvPEGgoOd+7506VLccccdeOaZZxATEwMAiIyMxNKlS6HVatGtWzcMHz4cWVlZDDdERMRhqUDRrVs3XHfddVi5ciUA4OjRo/j6668xYcIEOBwOPPnkk+jVqxfatGmDkJAQbNq0Cbm5ufVa9+HDhxEfH+8JNgCQnJxco93atWtx/fXXIzY2FiEhIXjiiSfqvY3q20pMTPQEGwC4/vrrIcsycnJyPNOuvvpqaLVaz+e4uDgUFBQ0aFtERBSY2HNzKXqzswelHqxnT8BoL8Jv2ii0je6gzLYbYMKECZg2bRqWLVuGVatWoXPnzrjpppvwzDPP4P/+7/+wZMkS9OrVC8HBwXj00Udhs9maXqNLdnY2xowZg/nz5yM1NRXh4eFYs2YNnn/+ecW2UZ1er/f6LEkSZN48kYiIwHBzaZJU76EhZzsbJI2p/sso6L777sP06dPxzjvv4I033sAjjzwCSZLw7bffYsSIEfjjH/8IwHkOzU8//YQePXrUa73du3dHXl4eTp8+jbi4OADAd99959Vm+/bt6NixI2bNmuWZdvLkSa82BoMBDkfd9wDq3r07Vq9ejbKyMk/vzbfffguNRoOrrrqqXvUSEVHrxmEpJal0tZRbSEgI0tLSMHPmTJw+fRrjx48HAHTt2hWbN2/G9u3bcfjwYfz5z39Gfn5+vdebkpKCK6+8EuPGjcO+ffvw9ddfe4UY9zZyc3OxZs0aHDt2DC+88ALWr1/v1SYhIQHHjx/H3r17cfbsWVit1hrbGjNmDEwmE8aNG4eDBw9i69atmDZtGh588EHP+TZERER1YbhRkOQKNxLUGx6ZMGECfv/9d6SmpnrOkXniiSfQt29fpKamYvDgwYiNjcXIkSPrvU6NRoP169ejoqICAwcOxEMPPYSnnnrKq82dd96Jv/71r5g6dSr69OmD7du3Y/bs2V5tRo0ahaFDh+Lmm29Gu3btar0c3Ww2Y9OmTTh37hwGDBiAe+65B0OGDMHSpUsb/sMgIqJWSRKidT3Curi4GOHh4SgqKkJYWJjXvMrKShw/fhxXXHEFTCZTg9dtKzwNQ7kFRQhFePsuSpVMCmjqsSUiInXV9f19IfbcKKkZ9NwQERG1dgw3CpI07nDTqjrDiIiImhWGGwVJGud9VzQqnVBMREREDDfKqjYs1cpOZSIiImo2GG5q0dhg4um5gQCzTfPCsElE1How3FTjvutteXk9H5R5AY3rnBsNZMj8Mm1W3Mf0wjsbExFR4OEdiqvRarWIiIjwPKPIbDZ7nrBdL/YqoEqgSsiQKyph0DE7qk0IgfLychQUFCAiIsLreVRERBSYGG4u4H5idaMewihXAcVnICDBUaKHTstw01xERETU+TRyIiIKHAw3F5AkCXFxcYiOjobdbm/YwpVFwGejAQDH7tmMK2IjfVAhNZRer2ePDRFRK8JwcxFarbbhX4g6DVCaBwBw2K28Ey4REZEKOG6iJJ0BdldetFeUqlwMERFR68RwozCrZAQA2K0MN0RERGpguFGYVXIORcmVJSpXQkRE1Dox3CjMqgkCAMjWxt0rh4iIiJqG4UZhNle4cXBYioiISBXNItwsW7YMCQkJMJlMSEpKwo4dOy7advXq1ZAkyevVnK5KqnKFG2EtU7kSIiKi1kn1cLN27VpkZGRg7ty52LNnDxITE5GamlrnTfTCwsJw+vRpz+vkyZN+rLhudp0r3NgYboiIiNSgerhZvHgxJk6ciPT0dPTo0QMrVqyA2WzGypUrL7qMJEmIjY31vGJiYvxYcd2qtM5wAxvPuSEiIlKDquHGZrNh9+7dSElJ8UzTaDRISUlBdnb2RZcrLS1Fx44dER8fjxEjRuDQoUMXbWu1WlFcXOz18iWHzgwAkOzsuSEiIlKDquHm7NmzcDgcNXpeYmJiYLFYal3mqquuwsqVK/HRRx/hrbfegizLuO666/DLL7/U2j4zMxPh4eGeV3x8vOL7UZ0n3FSx54aIiEgNqg9LNVRycjLGjh2LPn364KabbsIHH3yAdu3a4aWXXqq1/cyZM1FUVOR55eXl+bQ+4Qo3WjvDDRERkRpUfbZUVFQUtFot8vPzvabn5+fX+wnOer0e11xzDY4ePVrrfKPRCKPR2ORa60voXeGGPTdERESqULXnxmAwoF+/fsjKyvJMk2UZWVlZSE5Ortc6HA4HDhw4gLi4OF+V2TCGYACA1lGhciFEREStk+pPBc/IyMC4cePQv39/DBw4EEuWLEFZWRnS09MBAGPHjkWHDh2QmZkJAFiwYAGuvfZadOnSBYWFhVi0aBFOnjyJhx56SM3d8JAMzp4bHXtuiIiIVKF6uElLS8OZM2cwZ84cWCwW9OnTBxs3bvScZJybmwuN5nwH0++//46JEyfCYrEgMjIS/fr1w/bt29GjRw+1dsGLZHT23Ohl9twQERGpQRJCCLWL8Kfi4mKEh4ejqKgIYWFhiq9/z6a30Dd7Co7ouqPbE98pvn4iIqLWqCHf3y3uaqnmTmt0DksZBXtuiIiI1MBwozCtKQQAYJQrVa6EiIiodWK4UZguKBQAYGLPDRERkSoYbhSmNzlPKDbBqnIlRERErRPDjcL0Qc6TnEzCCsiyytUQERG1Pgw3CjO6hqU0kgCqODRFRETkbww3CjOagz3vqypLVayEiIiodWK4UZjJoEe5cD7LylpeonI1RERErQ/DjcKMOg1KEQQAsJUXqlsMERFRK8RwozBJklDmCjf2siKVqyEiImp9GG58oExynndjY7ghIiLyO4YbH6jUOB/BYOewFBERkd8x3PiAVet8BIOtnD03RERE/sZw4wN2nXNYSi4vVrkSIiKi1ofhxgfseueN/EQle26IiIj8jeHGB2SDc1hKWNlzQ0RE5G8MNz4gG5zPl9JYeRM/IiIif2O48QHJ6Hq+lJ2PXyAiIvI3hhsfkFxPBtcx3BAREfkdw40P6EzhAAB9FcMNERGRvzHc+IAu2BlujI4ylSshIiJqfRhufMBgjgAABMkMN0RERP7GcOMDxpAIAECQKAeEULcYIiKiVobhxgdMrnCjRxVQValuMURERK0Mw40PhIRGQBaS8wPvdUNERORXDDc+EBJkQClMAABHBR/BQERE5E8MNz4QbNSiBGYAQEXx7ypXQ0RE1Low3PiAUadFGYIAABWlDDdERET+xHDjIxWSs+fGWl6obiFEREStDMONj1RqnU8Gt5fxyeBERET+xHDjI1ZtMADAzp4bIiIiv2K48RG7ztlz46hgzw0REZE/Mdz4SJXeGW5EJcMNERGRPzHc+IisDwUASFaGGyIiIn9iuPERYXSHG96hmIiIyJ8YbnzFFAYA0NpLVS6EiIiodWG48RGNK9zo7Oy5ISIi8ieGGx/RmsMBAAZHmcqVEBERtS4MNz5icIUbo4PDUkRERP7EcOMj+uAIAIBJLle3ECIiolaG4cZHTK5wYxblgBDqFkNERNSKMNz4iCk0EgCggQBsHJoiIiLyF4YbHwkNDoFdaJ0feK8bIiIiv2G48ZEQkx4lCAIAVPHhmURERH7DcOMjISYdSoUz3FSWFqlcDRERUevBcOMjeq0GpVIwAKCi5JzK1RAREbUeDDc+VCGZAQDWMvbcEBER+UuzCDfLli1DQkICTCYTkpKSsGPHjnott2bNGkiShJEjR/q2wEaq1Dp7buw854aIiMhvVA83a9euRUZGBubOnYs9e/YgMTERqampKCgoqHO5EydO4LHHHsONN97op0obrkrn7LmpqihWuRIiIqLWQ/Vws3jxYkycOBHp6eno0aMHVqxYAbPZjJUrV150GYfDgTFjxmD+/Pno1KmTH6ttmCpXz01VBe9zQ0RE5C+qhhubzYbdu3cjJSXFM02j0SAlJQXZ2dkXXW7BggWIjo7GhAkTLrkNq9WK4uJir5e/OPTOcCPzPjdERER+o2q4OXv2LBwOB2JiYrymx8TEwGKx1LrMN998g9deew2vvPJKvbaRmZmJ8PBwzys+Pr7JddeXMIQ431jZc0NEROQvqg9LNURJSQkefPBBvPLKK4iKiqrXMjNnzkRRUZHnlZeX5+Mqz5OMznAj+PgFIiIiv9GpufGoqChotVrk5+d7Tc/Pz0dsbGyN9seOHcOJEydwxx13eKbJsgwA0Ol0yMnJQefOnb2WMRqNMBqNPqj+0jQmZ7jR2MpU2T4REVFrpGrPjcFgQL9+/ZCVleWZJssysrKykJycXKN9t27dcODAAezdu9fzuvPOO3HzzTdj7969fh1yqg+dKQwAoK1iuCEiIvIXVXtuACAjIwPjxo1D//79MXDgQCxZsgRlZWVIT08HAIwdOxYdOnRAZmYmTCYTevbs6bV8REQEANSY3hzog0IBALqqcpUrISIiaj1UDzdpaWk4c+YM5syZA4vFgj59+mDjxo2ek4xzc3Oh0bSoU4M8DGZnz43BwZ4bIiIif5GEEELtIvypuLgY4eHhKCoqQlhYmE+3dWDnNvT6ZAQKpLaInvs/n26LiIgokDXk+7tldom0EEHBEc4/RYW6hRAREbUiDDc+ZA51JssgUQm0rg4yIiIi1TDc+FBIWAQAQCfJqKzgeTdERET+wHDjQyEh4Z73pSWF6hVCRETUijDc+JBGq0U5nDcQLCspUrkaIiKi1oHhxsfKYQYAVLDnhoiIyC8YbnzMqgkCAFSU+e9p5ERERK0Zw42P2bTOcGMtZ7ghIiLyB4YbH7Nrg51/MtwQERH5BcONj1XpnOGmqoLhhoiIyB8YbnxM1ocAAByVJSpXQkRE1Dow3PiYMDivlhLWUpUrISIiah0YbnxMMoY639gYboiIiPyB4cbHNCZnuJEYboiIiPyC4cbHdCbnOTfaqnKVKyEiImodGG58TBfkfDK4rooPziQiIvIHhhsfM5idD880ONhzQ0RE5A8MNz5mCnaec2OUK1SuhIiIqHVguPExU4iz5yZIVMBa5VC5GiIiosDHcONj5uAIAECwVImSyip1iyEiImoFGG58zH0peAgqUFxhV7kaIiKiwMdw42sG57OlzKhEMXtuiIiIfI7hxteMzvvcGCQHSkp5Iz8iIiJfY7jxNUOI5215aZGKhRAREbUODDe+ptHCKhkBAJWlxSoXQ0REFPgYbvzApnE+Gdxazp4bIiIiX2O48QO7zhlubOXsuSEiIvI1hhs/qNI5r5iqqihRuRIiIqLAx3DjB7LeFW4qGW6IiIh8jeHGD9zhRrLyUnAiIiJfY7jxB9fl4Bo7ww0REZGvMdz4g+tGfpqqMpULISIiCnwMN36gMTqfL6Wzl6tcCRERUeBjuPEDrevhmXoHe26IiIh8jeHGD3RBzmEpvYM9N0RERL7GcOMH+qAwAIBRroAsC5WrISIiCmwMN35gcPXcmGBDhd2hcjVERESBjeHGD/QmZ7gxS1aUWatUroaIiCiwMdz4gWRwPlvKBCtKGW6IiIh8iuHGH/RBAIAg2FBu47AUERGRLzHc+IPe2XNjZs8NERGRzzHc+IMr3Jh4zg0REZHPMdz4Q7VhqTIOSxEREflUo8JNXl4efvnlF8/nHTt24NFHH8XLL7+sWGEBxdVzEwQryirtKhdDREQU2BoVbh544AFs3boVAGCxWHDrrbdix44dmDVrFhYsWKBogQHB1XOjlQQqK3iXYiIiIl9qVLg5ePAgBg4cCAB477330LNnT2zfvh1vv/02Vq9erWR9gcHVcwMA1go+X4qIiMiXGhVu7HY7jEYjAGDLli248847AQDdunXD6dOnlasuUGh1qJL0AAC7leGGiIjIlxoVbq6++mqsWLECX3/9NTZv3oyhQ4cCAE6dOoW2bds2eH3Lli1DQkICTCYTkpKSsGPHjou2/eCDD9C/f39EREQgODgYffr0wZtvvtmY3fCrKo0JAGCrLFW5EiIiosDWqHDzzDPP4KWXXsLgwYMxevRoJCYmAgA2bNjgGa6qr7Vr1yIjIwNz587Fnj17kJiYiNTUVBQUFNTavk2bNpg1axays7Oxf/9+pKenIz09HZs2bWrMrviNQ+c878ZRyZ4bIiIiX5KEEI16TLXD4UBxcTEiIyM9006cOAGz2Yzo6Oh6rycpKQkDBgzA0qVLAQCyLCM+Ph7Tpk3D448/Xq919O3bF8OHD8eTTz55ybbFxcUIDw9HUVERwsLC6l1nUxU/2xth5SexMO7/8Pifx/ttu0RERIGgId/fjeq5qaiogNVq9QSbkydPYsmSJcjJyWlQsLHZbNi9ezdSUlLOF6TRICUlBdnZ2ZdcXgiBrKws5OTkYNCgQbW2sVqtKC4u9nqpQbiumJJ5zg0REZFPNSrcjBgxAm+88QYAoLCwEElJSXj++ecxcuRILF++vN7rOXv2LBwOB2JiYrymx8TEwGKxXHS5oqIihISEwGAwYPjw4XjxxRdx66231to2MzMT4eHhnld8fHy961OUa1hK2HkpOBERkS81Ktzs2bMHN954IwDg/fffR0xMDE6ePIk33ngDL7zwgqIF1iY0NBR79+7Fzp078dRTTyEjIwPbtm2rte3MmTNRVFTkeeXl5fm8vtoI15PBwXBDRETkU7rGLFReXo7Q0FAAwOeff467774bGo0G1157LU6ePFnv9URFRUGr1SI/P99ren5+PmJjYy+6nEajQZcuXQAAffr0weHDh5GZmYnBgwfXaGs0Gj2XratJ4wo3kr1C5UqIiIgCW6N6brp06YIPP/wQeXl52LRpE2677TYAQEFBQYNO0jUYDOjXrx+ysrI802RZRlZWFpKTk+u9HlmWYbVa678DKvCEmyqGGyIiIl9qVM/NnDlz8MADD+Cvf/0rbrnlFk8Q+fzzz3HNNdc0aF0ZGRkYN24c+vfvj4EDB2LJkiUoKytDeno6AGDs2LHo0KEDMjMzATjPoenfvz86d+4Mq9WKTz/9FG+++WaDzvVRg9YVbnSOCgghIEmSyhUREREFpkaFm3vuuQc33HADTp8+7bnHDQAMGTIEd911V4PWlZaWhjNnzmDOnDmwWCzo06cPNm7c6DnJODc3FxrN+Q6msrIyTJ48Gb/88guCgoLQrVs3vPXWW0hLS2vMrviNzhQCADDCikq7jCCDVuWKiIiIAlOj73Pj5n46+GWXXaZIQb6m1n1uxOdzIW1fgleqbsfIGavRLlT984CIiIhaCp/f50aWZSxYsADh4eHo2LEjOnbsiIiICDz55JOQZblRRQc6yTUsFQQrym1VKldDREQUuBo1LDVr1iy89tprWLhwIa6//noAwDfffIN58+ahsrISTz31lKJFBgR3uJGsKLUy3BAREflKo8LN66+/jldffdXzNHAA6N27Nzp06IDJkycz3NTGdYfiINhQZnWoXAwREVHgatSw1Llz59CtW7ca07t164Zz5841uaiApD8/LFXGYSkiIiKfaVS4SUxM9DzosrqlS5eid+/eTS4qILl6bsySFWUcliIiIvKZRg1LPfvssxg+fDi2bNniucdNdnY28vLy8OmnnypaYMDQBwMATLAx3BAREflQo3pubrrpJvz000+46667UFhYiMLCQtx99904dOgQ3nzzTaVrDAyec26sPOeGiIjIhxrVcwMA7du3r3Hi8L59+/Daa6/h5ZdfbnJhAcdzzg17boiIiHypUT031AjunhvJilKeUExEROQzDDf+Uu0mfuy5ISIi8h2GG3+pNixVXslwQ0RE5CsNOufm7rvvrnN+YWFhU2oJbK5hKY0kUFlZoXIxREREgatB4SY8PPyS88eOHdukggKWq+cGABzWUhULISIiCmwNCjerVq3yVR2BT6OFQ2OAVrbBYS1XuxoiIqKAxXNu/EjWOYemHNYylSshIiIKXAw3fiRc4UbYGG6IiIh8heHGn1yXg8POE4qJiIh8heHGjyTXScWSg+GGiIjIVxhu/EhyXQ6ud1TCIQuVqyEiIgpMDDd+JBnP36W40s6HZxIREfkCw40faQzBAIAgyYZyG8MNERGRLzDc+JGkZ88NERGRrzHc+JP7yeCwoYLhhoiIyCcYbvzJMyxlRQWHpYiIiHyC4cafPD03Vp5zQ0RE5CMMN/7kOefGxnNuiIiIfIThxp/cPTeSlefcEBER+QjDjT9Vu1qK59wQERH5BsONP1UblmLPDRERkW8w3PhT9WEp9twQERH5BMONP1UflmLPDRERkU8w3PiTgcNSREREvsZw408cliIiIvI5hht/4tVSREREPsdw40+unhsTh6WIiIh8huHGn3TVHpxpq1K5GCIiosDEcONPrp4bjSRQZa9UuRgiIqLAxHDjT65wAwAOa7mKhRAREQUuhht/0uohSzoAgGyrULkYIiKiwMRw42dCZ3K+sTPcEBER+QLDjZ/JrpOKGW6IiIh8g+HG39w9N1UMN0RERL7AcONv7pOKGW6IiIh8guHGzyRXuNE5KuGQhcrVEBERBR6GGz/TuB6eaYIdlbxLMRERkeIYbvxMMrjvUmzlIxiIiIh8gOHGzyTXwzNNko0PzyQiIvKBZhFuli1bhoSEBJhMJiQlJWHHjh0XbfvKK6/gxhtvRGRkJCIjI5GSklJn+2bHdbUUH55JRETkG6qHm7Vr1yIjIwNz587Fnj17kJiYiNTUVBQUFNTaftu2bRg9ejS2bt2K7OxsxMfH47bbbsOvv/7q58obSV/94ZkMN0REREpTPdwsXrwYEydORHp6Onr06IEVK1bAbDZj5cqVtbZ/++23MXnyZPTp0wfdunXDq6++ClmWkZWV5efKG8kVbowSe26IiIh8QdVwY7PZsHv3bqSkpHimaTQapKSkIDs7u17rKC8vh91uR5s2bWqdb7VaUVxc7PVSVfWeG4YbIiIixakabs6ePQuHw4GYmBiv6TExMbBYLPVax4wZM9C+fXuvgFRdZmYmwsPDPa/4+Pgm190krscvmDgsRURE5BOqD0s1xcKFC7FmzRqsX78eJpOp1jYzZ85EUVGR55WXl+fnKi+gr3YpOMMNERGR4nRqbjwqKgparRb5+fle0/Pz8xEbG1vnss899xwWLlyILVu2oHfv3hdtZzQaYTQaFalXEa5wY5JsKOKwFBERkeJU7bkxGAzo16+f18nA7pODk5OTL7rcs88+iyeffBIbN25E//79/VGqcqpdCs47FBMRESlP1Z4bAMjIyMC4cePQv39/DBw4EEuWLEFZWRnS09MBAGPHjkWHDh2QmZkJAHjmmWcwZ84cvPPOO0hISPCcmxMSEoKQkBDV9qPe3Dfxgw3lHJYiIiJSnOrhJi0tDWfOnMGcOXNgsVjQp08fbNy40XOScW5uLjSa8x1My5cvh81mwz333OO1nrlz52LevHn+LL1x9M6emyBeCk5EROQTqocbAJg6dSqmTp1a67xt27Z5fT5x4oTvC/IlPa+WIiIi8qUWfbVUi6Q7f7UUz7khIiJSHsONv3nuUGznsBQREZEPMNz4W7X73PCEYiIiIuUx3PhbtXNuOCxFRESkPIYbf9NVe7aUtUrlYoiIiAIPw42/uXpuNJKA3W5VuRgiIqLAw3Djb65wAwDCVq5iIURERIGJ4cbftHoIyXl7IWGvULkYIiKiwMNwowLher4U7Oy5ISIiUhrDjQqEe2jKXqluIURERAGI4UYNrnCjdVRCloXKxRAREQUWhhsVSO573fDhmURERIpjuFGBJ9zAynBDRESkMIYbFUh6MwDABDufDE5ERKQwhhs16J1XS/HJ4ERERMpjuFGDu+dGsqGMPTdERESKYrhRg87dc2NDOZ8vRUREpCiGGzVUezI4e26IiIiUxXCjBle4MUo2lNvYc0NERKQkhhs1uMJNEGwo5bAUERGRohhu1KA7PyxVbuWwFBERkZIYbtRQ7VLwMg5LERERKYrhRg3VLwXnsBQREZGiGG7U4LoUnFdLERERKY/hRg2exy/wPjdERERKY7hRg/ucG96hmIiISHEMN2qofhM/9twQEREpiuFGDTr3fW6s7LkhIiJSGMONGjx3KLbznBsiIiKFMdyoQV+t54bhhoiISFEMN2rggzOJiIh8huFGDbrzz5Yqt9lVLoaIiCiwMNyowdVzo5EEJIcd1ir23hARESmF4UYNrnADACZY+fBMIiIiBTHcqEGrBzQ6AIAJdj48k4iISEEMN2pxn3cjWVHGnhsiIiLFMNyoRX/+pGL23BARESmH4UYthmAAgBmVPOeGiIhIQQw3ajGFAwDCpDL23BARESmI4UYtpjAAQBgqeJdiIiIiBTHcqMXoDDehUjnvUkxERKQghhu1mCIAAGEo58MziYiIFMRwoxZTtZ4bhhsiIiLFMNyoxeg+56aMw1JEREQKYrhRi+tqqVCpAuW8WoqIiEgxDDdqcQ9LoRylvM8NERGRYhhu1OIelpJ4QjEREZGSVA83y5YtQ0JCAkwmE5KSkrBjx46Ltj106BBGjRqFhIQESJKEJUuW+K9QpbmHpVDOm/gREREpSNVws3btWmRkZGDu3LnYs2cPEhMTkZqaioKCglrbl5eXo1OnTli4cCFiY2P9XK3CvK6W4rAUERGRUlQNN4sXL8bEiRORnp6OHj16YMWKFTCbzVi5cmWt7QcMGIBFixbh/vvvh9Fo9HO1CnM/foE9N0RERIpSLdzYbDbs3r0bKSkp54vRaJCSkoLs7GzFtmO1WlFcXOz1ahaMznATIlXCWmlTuRgiIqLAoVq4OXv2LBwOB2JiYrymx8TEwGKxKLadzMxMhIeHe17x8fGKrbtJXMNSAAB7iXp1EBERBRjVTyj2tZkzZ6KoqMjzysvLU7skJ60eQhfkfGsthhBC5YKIiIgCg06tDUdFRUGr1SI/P99ren5+vqInCxuNxmZ7fo4whUEqrUAIymGtkmHSa9UuiYiIqMVTrefGYDCgX79+yMrK8kyTZRlZWVlITk5Wqyy/ktwnFfP5UkRERIpRrecGADIyMjBu3Dj0798fAwcOxJIlS1BWVob09HQAwNixY9GhQwdkZmYCcJ6E/OOPP3re//rrr9i7dy9CQkLQpUsX1fajsaTq97qxOtA2ROWCiIiIAoCq4SYtLQ1nzpzBnDlzYLFY0KdPH2zcuNFzknFubi40mvOdS6dOncI111zj+fzcc8/hueeew0033YRt27b5u/ymM55/BAMvByciIlKGquEGAKZOnYqpU6fWOu/CwJKQkBBYJ96aqj2CgeGGiIhIEQF/tVSzVm1Yig/PJCIiUgbDjZrcw1JSBR+eSUREpBCGGzV5HsFQhjIbe26IiIiUwHCjJvewFC8FJyIiUgzDjZpcw1JhKEdxhV3lYoiIiAIDw42aqvXcnC21qlwMERFRYGC4UZPJfZ+bChSUMNwQEREpgeFGTe5hKakMZxhuiIiIFMFwoybP1VIVOMNhKSIiIkUw3KjJNSxllOwoKilRuRgiIqLAwHCjJkMoBCQAgM5WilJeDk5ERNRkDDdq0mggee5SXM7zboiIiBTAcKM298MzwZOKiYiIlMBwo7Zqz5cqKKlUuRgiIqKWj+FGbdWeDM6eGyIioqZjuFGbe1iK59wQEREpguFGba6emwiU8i7FRERECmC4UVtkAgAgQbKw54aIiEgBDDdqa3cVAKCr5leGGyIiIgUw3KitXTcAQFfpF5zh1VJERERNxnCjtrZdICQNwqVyaMry4ZCF2hURERG1aAw3atMZgTadAACdpV/xWxmHpoiIiJqC4aYZkDxDUzzvhoiIqKkYbpoD90nF0i+8HJyIiKiJGG6aA3fPDa+YIiIiajKGm+bA1XPThcNSRERETcZw0xy07QoBCW2lEpSds6hdDRERUYvGcNMcGMwoCeoAANCf+0nlYoiIiFo2hptmwhbZFQBgPf0jqhyyytUQERG1XAw3zURkx14AgFjbSXz50xmVqyEiImq5GG6aCW2cM9zcoc3GF9m7VK6GiIio5WK4aS6634GKqF5oK5Xgjycfx7nfz6ldERERUYvEcNNc6IMQ9OAa/C5FoLuUi9K3xwG2crWrIiIianEYbpqT8MuQPeAFWIUOl5/9ChUvpQC/n1C7KiIiohaF4aaZuW7wMEzWzMZZEYag3w6hbOkgVP6yX+2yiIiIWgyGm2YmwmzA3KmTsKTTK9gvX4FgRxEqVo6A9cz/1C6NiIioRWC4aYYub2vGv8YNRcX9/w8/iXhEyufw+0t/gLUoX+3SiIiImj2Gm2YsqUdnlNz7Hn4VUYit+hWn/3MH7OVFapdFRETUrDHcNHP9evaA5c53cU6EIsGag6NL70KVrVLtsoiIiJothpsWoF+/gfhf6mqUCSO6l+/GvqUPwOFwqF0WERFRs8Rw00L0vy4FR25aDrvQol9xFrYvmwiZz6AiIiKqgeGmBel3yygcGrgQAHDjuf+Hz196DDY7e3CIiIiqY7hpYfoMn4QDvWYCAIYWvIbvF41E3imLylURERE1Hww3LVCvUY/jp8SZsEOLG21fQXrpBmx+IxPnfv9d7dKIiIhUJwkhhNpF+FNxcTHCw8NRVFSEsLAwtctpkoLD30Cs+xNiZOf9b4pEMHLCroPofAsSEm9CzOVXAlq9ylUSERE1XUO+vxluWji5ohg5G/+DiAOrECd7D09VQYNzulgUBV0Oa+hl0Ae3gSm0DfTBETAER8IYEuH8bApxhiCNFtDoAI0e0BkBncn50rCDj4iI1MVwU4dACzduwlGFo7s+x+/7NyIyfzs62HNhlqyKrNsuGVClMUCWdJAlHYSkhdDoIEt6CI3zvdC4w5EeQqMDNDpIWr0rNDnfS1o9JPd7nR4ajc453z1dAjSyDZLsgKSttoxWD0lngKTVQ+NZZy09UpIESFpnGJPcQU3rPU3SONtBumAZ9zwNIARgLwMcNsAYDpjCnfOFAIQMwPWnEM731bel0Z3fhqRxbsfrMy4xX/KuUXK9iIhauYZ8f+v8VFOdli1bhkWLFsFisSAxMREvvvgiBg4ceNH269atw+zZs3HixAl07doVzzzzDG6//XY/Vtz8SFoduibdDiQ5fw7lVjt2/fQzzuUdhnz2KDTFv0CuKILGWgSjowxmuRTBohyhKIdZqoQODmghQw8HdKiCVjqfefXCBr3DptaukYtwBTLnnxKEBODCaZA8uc3zGRKEV0Byf5a8l5eqLwNPwBKe9/AsI6oHRNeyotr6qocz7+ne65A8266+Ppxff43t1L6+mvW432s8k7y3c8H+Vv/5XGTb3jWd/1leGJYFJEjuPy/cv3psp/o63MdGunCaBEio2c69Lkk4oHHYAMiApIXQuH4RkXSAJLmPquuH7QrpQI3p7jV7f642DcK1vGvfNXoIjR5SVSU0VRUQGi1knRlC0kFyWCGJKshaI4TWCI1cBY2j0rmc1uj8JchzWKutt4ku/rvBhTMu1vD8/nsO80Vqc/87k3DhcXav6fz7+v7K4nVMmky64FMt61bodykpPB6hgx5RZmWNoHq4Wbt2LTIyMrBixQokJSVhyZIlSE1NRU5ODqKjo2u03759O0aPHo3MzEz84Q9/wDvvvIORI0diz5496Nmzpwp70DyZjXr079UD6NXjom2qHDLKrA6U2qpQWlmFUqvzVWatQll5JSoqyuCwlUPYKgFHJRxVdgiHHbLDDlFVBeGwQTiqIBzO6ZDtgOyAcNghyXZArgLkKkjV/tTIdkA4oJGroEUVtKIKWuGAFg4IIWCDHlXQQgeH61XlDFySA3r3ezjfi1r+oWohQyvJ0EB2vsf59xrI0EBU/8qHBOGZ7l5OhgYVwogqaBGCCoRJZZAgPEsKSJCrhQANZOggQwsHdJBdc53TJQAaSaH/pC/8ArrYaltVXywRNUdHdN3RTcVwo/qwVFJSEgYMGIClS5cCAGRZRnx8PKZNm4bHH3+8Rvu0tDSUlZXh448/9ky79tpr0adPH6xYseKS2wvUYalAIYSAQxZwCAEhcP69DDhc84QQ1d5XayMEHLLzsyzcL3iWkQVq/gnnn7JreVl2ZgPPZ9cvkO71CfcysqsNUKOdcE2XZfc2ACHLzumywzXfOawlCed0uOYDrj+rD3sJASFkzy+y7nZwbQfubXqWg2sb4vw8uAqBDCG7f1MXrnUK1zIyJNc6JbjW516Hazvu5STXz8H5Azm/LVeFztE69764apYgPPPgWv/5oCZcAdIVGUX1tu5A5wyL7nV6hT2vaa71ev5nqz7tfI0SaushcMXX2nozPDWeD8bO/RSuX+qFZ73ubQLO3/LdUdhr2Wo/D6natqv3hlSf7unbEcK77QXtIAAHNLBLesiQoIUDWuHqkYXs3hUv1bcgvKZV/2kBQlSf5r2sBNn1C0gVbDCiUjJCAxlmOHuGrTDAAS0MsMEAG+xCh0oYADh7h3VwXPATvESfRQO+udw/99rUp2ektpq8e2FEre+rf77kduqYfeEvcg350m7qF3yNbddzhZqIyzE+I7OJW/fWYoalbDYbdu/ejZkzZ3qmaTQapKSkIDs7u9ZlsrOzkZGR4TUtNTUVH374Ya3trVYrrNbz554UFxc3vXDyGUmSoNNK6ncpEhFRi6XqZTBnz56Fw+FATEyM1/SYmBhYLLXfmM5isTSofWZmJsLDwz2v+Ph4ZYonIiKiZingr/GdOXMmioqKPK+8vDy1SyIiIiIfUrX3PyoqClqtFvn5+V7T8/PzERsbW+sysbGxDWpvNBphNBqVKZiIiIiaPVV7bgwGA/r164esrCzPNFmWkZWVheTk5FqXSU5O9moPAJs3b75oeyIiImpdVD9vMyMjA+PGjUP//v0xcOBALFmyBGVlZUhPTwcAjB07Fh06dEBmpvOs6+nTp+Omm27C888/j+HDh2PNmjXYtWsXXn75ZTV3g4iIiJoJ1cNNWloazpw5gzlz5sBisaBPnz7YuHGj56Th3NxcaKrd/v+6667DO++8gyeeeAL//Oc/0bVrV3z44Ye8xw0REREBaAb3ufE33ueGiIio5WnI93fAXy1FRERErQvDDREREQUUhhsiIiIKKAw3REREFFAYboiIiCigMNwQERFRQGG4ISIiooCi+k38/M19W5/i4mKVKyEiIqL6cn9v1+f2fK0u3JSUlAAA4uPjVa6EiIiIGqqkpATh4eF1tml1dyiWZRmnTp1CaGgoJElSdN3FxcWIj49HXl5eQN79OND3D+A+BoJA3z+A+xgIAn3/AOX3UQiBkpIStG/f3uuxTLVpdT03Go0Gl112mU+3ERYWFrB/WYHA3z+A+xgIAn3/AO5jIAj0/QOU3cdL9di48YRiIiIiCigMN0RERBRQGG4UZDQaMXfuXBiNRrVL8YlA3z+A+xgIAn3/AO5jIAj0/QPU3cdWd0IxERERBTb23BAREVFAYbghIiKigMJwQ0RERAGF4YaIiIgCCsONQpYtW4aEhASYTCYkJSVhx44dapfUaJmZmRgwYABCQ0MRHR2NkSNHIicnx6vN4MGDIUmS1+vhhx9WqeKGmTdvXo3au3Xr5plfWVmJKVOmoG3btggJCcGoUaOQn5+vYsUNl5CQUGMfJUnClClTALTM4/fVV1/hjjvuQPv27SFJEj788EOv+UIIzJkzB3FxcQgKCkJKSgp+/vlnrzbnzp3DmDFjEBYWhoiICEyYMAGlpaV+3IuLq2v/7HY7ZsyYgV69eiE4OBjt27fH2LFjcerUKa911HbcFy5c6Oc9ubhLHcPx48fXqH/o0KFebZrzMQQuvY+1/buUJAmLFi3ytGnOx7E+3w/1+T80NzcXw4cPh9lsRnR0NP7+97+jqqpKsToZbhSwdu1aZGRkYO7cudizZw8SExORmpqKgoICtUtrlC+//BJTpkzBd999h82bN8Nut+O2225DWVmZV7uJEyfi9OnTntezzz6rUsUNd/XVV3vV/s0333jm/fWvf8V///tfrFu3Dl9++SVOnTqFu+++W8VqG27nzp1e+7d582YAwL333utp09KOX1lZGRITE7Fs2bJa5z/77LN44YUXsGLFCnz//fcIDg5GamoqKisrPW3GjBmDQ4cOYfPmzfj444/x1VdfYdKkSf7ahTrVtX/l5eXYs2cPZs+ejT179uCDDz5ATk4O7rzzzhptFyxY4HVcp02b5o/y6+VSxxAAhg4d6lX/u+++6zW/OR9D4NL7WH3fTp8+jZUrV0KSJIwaNcqrXXM9jvX5frjU/6EOhwPDhw+HzWbD9u3b8frrr2P16tWYM2eOcoUKarKBAweKKVOmeD47HA7Rvn17kZmZqWJVyikoKBAAxJdffumZdtNNN4np06erV1QTzJ07VyQmJtY6r7CwUOj1erFu3TrPtMOHDwsAIjs7208VKm/69Omic+fOQpZlIUTLPn5CCAFArF+/3vNZlmURGxsrFi1a5JlWWFgojEajePfdd4UQQvz4448CgNi5c6enzWeffSYkSRK//vqr32qvjwv3rzY7duwQAMTJkyc90zp27Cj+/e9/+7Y4hdS2j+PGjRMjRoy46DIt6RgKUb/jOGLECHHLLbd4TWtJx/HC74f6/B/66aefCo1GIywWi6fN8uXLRVhYmLBarYrUxZ6bJrLZbNi9ezdSUlI80zQaDVJSUpCdna1iZcopKioCALRp08Zr+ttvv42oqCj07NkTM2fORHl5uRrlNcrPP/+M9u3bo1OnThgzZgxyc3MBALt374bdbvc6nt26dcPll1/eYo+nzWbDW2+9hT/96U9eD4ttycfvQsePH4fFYvE6buHh4UhKSvIct+zsbERERKB///6eNikpKdBoNPj+++/9XnNTFRUVQZIkREREeE1fuHAh2rZti2uuuQaLFi1StKvfH7Zt24bo6GhcddVVeOSRR/Dbb7955gXaMczPz8cnn3yCCRMm1JjXUo7jhd8P9fk/NDs7G7169UJMTIynTWpqKoqLi3Ho0CFF6mp1D85U2tmzZ+FwOLwOEgDExMTgyJEjKlWlHFmW8eijj+L6669Hz549PdMfeOABdOzYEe3bt8f+/fsxY8YM5OTk4IMPPlCx2vpJSkrC6tWrcdVVV+H06dOYP38+brzxRhw8eBAWiwUGg6HGF0ZMTAwsFos6BTfRhx9+iMLCQowfP94zrSUfv9q4j01t/w7d8ywWC6Kjo73m63Q6tGnTpsUd28rKSsyYMQOjR4/2eiDhX/7yF/Tt2xdt2rTB9u3bMXPmTJw+fRqLFy9Wsdr6Gzp0KO6++25cccUVOHbsGP75z39i2LBhyM7OhlarDahjCACvv/46QkNDawx7t5TjWNv3Q33+D7VYLLX+W3XPUwLDDdVpypQpOHjwoNc5KQC8xrh79eqFuLg4DBkyBMeOHUPnzp39XWaDDBs2zPO+d+/eSEpKQseOHfHee+8hKChIxcp847XXXsOwYcPQvn17z7SWfPxaO7vdjvvuuw9CCCxfvtxrXkZGhud97969YTAY8Oc//xmZmZkt4jb/999/v+d9r1690Lt3b3Tu3Bnbtm3DkCFDVKzMN1auXIkxY8bAZDJ5TW8px/Fi3w/NAYelmigqKgparbbGmeD5+fmIjY1VqSplTJ06FR9//DG2bt2Kyy67rM62SUlJAICjR4/6ozRFRURE4Morr8TRo0cRGxsLm82GwsJCrzYt9XiePHkSW7ZswUMPPVRnu5Z8/AB4jk1d/w5jY2NrnORfVVWFc+fOtZhj6w42J0+exObNm716bWqTlJSEqqoqnDhxwj8FKqxTp06Iiory/L0MhGPo9vXXXyMnJ+eS/zaB5nkcL/b9UJ//Q2NjY2v9t+qepwSGmyYyGAzo168fsrKyPNNkWUZWVhaSk5NVrKzxhBCYOnUq1q9fjy+++AJXXHHFJZfZu3cvACAuLs7H1SmvtLQUx44dQ1xcHPr16we9Xu91PHNycpCbm9sij+eqVasQHR2N4cOH19muJR8/ALjiiisQGxvrddyKi4vx/fffe45bcnIyCgsLsXv3bk+bL774ArIse8Jdc+YONj///DO2bNmCtm3bXnKZvXv3QqPR1BjKaSl++eUX/Pbbb56/ly39GFb32muvoV+/fkhMTLxk2+Z0HC/1/VCf/0OTk5Nx4MABr6DqDus9evRQrFBqojVr1gij0ShWr14tfvzxRzFp0iQRERHhdSZ4S/LII4+I8PBwsW3bNnH69GnPq7y8XAghxNGjR8WCBQvErl27xPHjx8VHH30kOnXqJAYNGqRy5fXzt7/9TWzbtk0cP35cfPvttyIlJUVERUWJgoICIYQQDz/8sLj88svFF198IXbt2iWSk5NFcnKyylU3nMPhEJdffrmYMWOG1/SWevxKSkrEDz/8IH744QcBQCxevFj88MMPnquFFi5cKCIiIsRHH30k9u/fL0aMGCGuuOIKUVFR4VnH0KFDxTXXXCO+//578c0334iuXbuK0aNHq7VLXuraP5vNJu68805x2WWXib1793r9u3RfXbJ9+3bx73//W+zdu1ccO3ZMvPXWW6Jdu3Zi7NixKu/ZeXXtY0lJiXjsscdEdna2OH78uNiyZYvo27ev6Nq1q6isrPSsozkfQyEu/fdUCCGKioqE2WwWy5cvr7F8cz+Ol/p+EOLS/4dWVVWJnj17ittuu03s3btXbNy4UbRr107MnDlTsToZbhTy4osvissvv1wYDAYxcOBA8d1336ldUqMBqPW1atUqIYQQubm5YtCgQaJNmzbCaDSKLl26iL///e+iqKhI3cLrKS0tTcTFxQmDwSA6dOgg0tLSxNGjRz3zKyoqxOTJk0VkZKQwm83irrvuEqdPn1ax4sbZtGmTACBycnK8prfU47d169Za/16OGzdOCOG8HHz27NkiJiZGGI1GMWTIkBr7/ttvv4nRo0eLkJAQERYWJtLT00VJSYkKe1NTXft3/Pjxi/673Lp1qxBCiN27d4ukpCQRHh4uTCaT6N69u3j66ae9goHa6trH8vJycdttt4l27doJvV4vOnbsKCZOnFjjl8TmfAyFuPTfUyGEeOmll0RQUJAoLCyssXxzP46X+n4Qon7/h544cUIMGzZMBAUFiaioKPG3v/1N2O12xeqUXMUSERERBQSec0NEREQBheGGiIiIAgrDDREREQUUhhsiIiIKKAw3REREFFAYboiIiCigMNwQERFRQGG4IaJWT5IkfPjhh2qXQUQKYbghIlWNHz8ekiTVeA0dOlTt0oiohdKpXQAR0dChQ7Fq1SqvaUajUaVqiKilY88NEanOaDQiNjbW6xUZGQnAOWS0fPlyDBs2DEFBQejUqRPef/99r+UPHDiAW265BUFBQWjbti0mTZqE0tJSrzYrV67E1VdfDaPRiLi4OEydOtVr/tmzZ3HXXXfBbDaja9eu2LBhg293moh8huGGiJq92bNnY9SoUdi3bx/GjBmD+++/H4cPHwYAlJWVITU1FZGRkdi5cyfWrVuHLVu2eIWX5cuXY8qUKZg0aRIOHDiADRs2oEuXLl7bmD9/Pu677z7s378ft99+O8aMGYNz5875dT+JSCGKPYKTiKgRxo0bJ7RarQgODvZ6PfXUU0II51OIH374Ya9lkpKSxCOPPCKEEOLll18WkZGRorS01DP/k08+ERqNxvNE6fbt24tZs2ZdtAYA4oknnvB8Li0tFQDEZ599pth+EpH/8JwbIlLdzTffjOXLl3tNa9Omjed9cnKy17zk5GTs3bsXAHD48GEkJiYiODjYM//666+HLMvIycmBJEk4deoUhgwZUmcNvXv39rwPDg5GWFgYCgoKGrtLRKQihhsiUl1wcHCNYSKlBAUF1audXq/3+ixJEmRZ9kVJRORjPOeGiJq97777rsbn7t27AwC6d++Offv2oayszDP/22+/hUajwVVXXYXQ0FAkJCQgKyvLrzUTkXrYc0NEqrNarbBYLF7TdDodoqKiAADr1q1D//79ccMNN+Dtt9/Gjh078NprrwEAxowZg7lz52LcuHGYN28ezpw5g2nTpuHBBx9ETEwMAGDevHl4+OGHER0djWHDhqGkpATffvstpk2b5t8dJSK/YLghItVt3LgRcXFxXtOuuuoqHDlyBIDzSqY1a9Zg8uTJiIuLw7vvvosePXoAAMxmMzZt2oTp06djwIABMJvNGDVqFBYvXuxZ17hx41BZWYl///vfeOyxxxAVFYV77rnHfztIRH4lCSGE2kUQEV2MJElYv349Ro4cqXYpRNRC8JwbIiIiCigMN0RERBRQeM4NETVrHDknooZizw0REREFFIYbIiIiCigMN0RERBRQGG6IiIgooDDcEBERUUBhuCEiIqKAwnBDREREAYXhhoiIiAIKww0REREFlP8PQA6kWkY1Z5oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
